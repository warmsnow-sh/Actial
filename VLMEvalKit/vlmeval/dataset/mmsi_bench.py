import os
import re
import pandas as pd
import os.path as osp
import json
import numpy as np
import warnings
from tqdm import tqdm
from .image_mcq import ImageMCQDataset
from .utils import DEBUG_MESSAGE, build_judge
from ..smp import (LMUDataRoot, file_size, load, dump, decode_base64_to_image_file,
                   listinstr, gpt_key_set)
import string
import glob


class MMSIBenchDataset(ImageMCQDataset):
    """
    MMSI Bench Dataset class for multiple-choice questions with multiple images.
    æ”¯æŒå¤šå›¾ç‰‡çš„å¤šé€‰é¢˜è¯„æµ‹æ•°æ®é›†ï¼Œå›¾ç‰‡ä»¥JSONæ•°ç»„æ ¼å¼å­˜å‚¨åœ¨imageå­—æ®µä¸­ã€‚
    """
    TYPE = 'MCQ'

    DATASET_URL = {
        'MMSI_Bench': 'https://huggingface.co/datasets/RunsenXu/MMSI-Bench/resolve/main/MMSI_bench.tsv'
    }
    DATASET_MD5 = {
        'MMSI_Bench': 'c473f72a345f616fa68a628580b573b6'
    }

    @classmethod
    def supported_datasets(cls):
        return ['MMSI_Bench']

    def dump_image(self, line):
        """
        å¤„ç†å›¾ç‰‡å­—æ®µï¼Œæ”¯æŒå¤šå¼ å›¾ç‰‡ã€‚
        å¦‚æœimageå­—æ®µæ˜¯JSONæ•°ç»„æ ¼å¼ï¼Œåˆ™è§£æå¹¶å¤„ç†æ¯å¼ å›¾ç‰‡ã€‚
        """
        # å¤„ç†image_pathå­—æ®µ
        if 'image_path' in line and isinstance(line['image_path'], str):
            tgt_path = line['image_path']
            if not isinstance(tgt_path, list):
                tgt_path = [tgt_path]
            return tgt_path

        # å¤„ç†imageå­—æ®µ
        if 'image' in line:
            # è·å–imageå­—æ®µçš„å€¼ï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
            img_field = line['image']
            if isinstance(img_field, (pd.Series, np.ndarray)):
                # å¦‚æœæ˜¯Seriesæˆ–æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if len(img_field) > 0:
                    img_str = (img_field.iloc[0] if hasattr(img_field, 'iloc')
                               else img_field[0])
                else:
                    return None
            else:
                img_str = img_field

            # å¤„ç†å·²ç»æ˜¯åˆ—è¡¨ç±»å‹çš„å›¾ç‰‡æ•°æ®
            if isinstance(img_str, list):
                paths = []
                # å¤„ç†æ¯å¼ å›¾ç‰‡
                for i, img_base64 in enumerate(img_str):
                    img_path = os.path.join(self.img_root, f"{line['index']}_{i}.jpg")
                    decode_base64_to_image_file(img_base64, img_path)
                    paths.append(img_path)
                return paths

            # ç¡®ä¿img_stræ˜¯å­—ç¬¦ä¸²
            if not isinstance(img_str, str):
                return None

            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ•°ç»„æ ¼å¼çš„å¤šå›¾ç‰‡
            if img_str.startswith('[') and img_str.endswith(']'):
                try:
                    # å°è¯•è§£æJSONæ•°ç»„
                    img_list = json.loads(img_str)
                    paths = []

                    # å¤„ç†æ¯å¼ å›¾ç‰‡
                    for i, img_base64 in enumerate(img_list):
                        img_path = os.path.join(self.img_root, f"{line['index']}_{i}.jpg")
                        decode_base64_to_image_file(img_base64, img_path)
                        paths.append(img_path)

                    return paths
                except json.JSONDecodeError:
                    # å¦‚æœè§£æå¤±è´¥ï¼ŒæŒ‰å•å›¾ç‰‡å¤„ç†
                    pass

            # å•å›¾ç‰‡å¤„ç†
            img_path = os.path.join(self.img_root, f"{line['index']}.jpg")
            decode_base64_to_image_file(img_str, img_path)
            return img_path

        return None

    def build_prompt(self, line):
        """
        æ„å»ºæç¤ºï¼Œæ”¯æŒå¤šå›¾ç‰‡è¾“å…¥ã€‚
        åœ¨æ–°çš„TSVæ ¼å¼ä¸­ï¼Œé€‰é¡¹å·²ç»åŒ…å«åœ¨questionå­—æ®µä¸­ï¼Œä¸éœ€è¦å†ä»å•ç‹¬çš„åˆ—æå–é€‰é¡¹ã€‚
        """
        if isinstance(line, int):
            line = self.data.iloc[line]

        # å¤„ç†å›¾ç‰‡ï¼ˆæ”¯æŒå¤šå›¾ï¼‰
        tgt_path = self.dump_image(line)

        # æ„å»ºæ–‡æœ¬æç¤º - åœ¨æ–°æ ¼å¼ä¸­ï¼Œquestionå­—æ®µå·²ç»åŒ…å«äº†é€‰é¡¹ï¼Œä¸éœ€è¦å†æ‹¼æ¥
        question = line['question']
        # æ·»åŠ post_promptï¼Œå¼•å¯¼æ¨¡å‹ä»¥æ­£ç¡®æ ¼å¼å›ç­”
        post_prompt = ("Answer with the option's letter from the given choices directly. "
                       "Enclose the option's letter within ``.")
        prompt = f'{question}\n{post_prompt}'

        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
        msgs = []
        if isinstance(tgt_path, list):
            # å¤„ç†å¤šå¼ å›¾ç‰‡
            msgs.extend([dict(type='image', value=p) for p in tgt_path])
        else:
            # å¤„ç†å•å¼ å›¾ç‰‡
            msgs = [dict(type='image', value=tgt_path)]

        # æ·»åŠ æ–‡æœ¬æç¤º
        msgs.append(dict(type='text', value=prompt))
        return msgs

    @classmethod
    def evaluate(cls, eval_file, **judge_kwargs):
        """
        è¯„ä¼°æ¨¡å‹é¢„æµ‹ç»“æœã€‚
        ä½¿ç”¨extract_single_choice_with_word_boundaryå‡½æ•°æå–é¢„æµ‹çš„é€‰é¡¹ã€‚
        """
        data = load(eval_file)

        # ç¡®ä¿é¢„æµ‹å€¼å’Œç­”æ¡ˆéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
        data['prediction'] = [str(x) if x is not None else None for x in data['prediction']]
        data['answer'] = [str(x) if x is not None else None for x in data['answer']]

        # è®¡ç®—å‡†ç¡®ç‡
        correct = 0
        total = 0

        # æ·»åŠ é¢„æµ‹ç»“æœåˆ—
        data['extracted_pred'] = None
        data['score'] = 0.0

        for idx, row in data.iterrows():
            gt = row['answer']
            pred = row['prediction']

            # ä½¿ç”¨æä¾›çš„å‡½æ•°æå–é€‰é¡¹
            extracted_pred = cls.extract_single_choice_with_word_boundary(pred)

            # è®°å½•æå–çš„é¢„æµ‹ç»“æœ
            data.at[idx, 'extracted_pred'] = extracted_pred

            # å¦‚æœæå–åˆ°äº†æœ‰æ•ˆé€‰é¡¹ï¼Œè¿›è¡Œå¾—åˆ†è®¡ç®—
            if extracted_pred is not None:
                answer = gt.lower().replace("\n", " ").strip()
                predict = extracted_pred.lower().replace("\n", " ").strip()
                try:
                    if answer == predict[0]:
                        data.at[idx, 'score'] = 1.0
                        correct += 1
                    elif predict[0] == "(" and answer == predict[1]:
                        data.at[idx, 'score'] = 1.0
                        correct += 1
                    elif predict[0:7] == "option " and answer == predict[7]:
                        data.at[idx, 'score'] = 1.0
                        correct += 1
                    elif predict[0:14] == "the answer is " and answer == predict[14]:
                        data.at[idx, 'score'] = 1.0
                        correct += 1
                except Exception:
                    pass

            total += 1

        accuracy = correct / total if total > 0 else 0
        print("MMSI_Bench è¯„æµ‹ç»“æœï¼š")
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"æ­£ç¡®æ ·æœ¬æ•°: {correct}")
        print(f"å‡†ç¡®ç‡: {accuracy:.2%}")

        # åˆ†ç±»åˆ«è®¡ç®—å‡†ç¡®ç‡
        category_acc = {}
        if 'category' in data.columns:
            for category in data['category'].unique():
                cat_data = data[data['category'] == category]
                cat_correct = sum(cat_data['score'] == 1.0)
                cat_total = len(cat_data)

                category_acc[category] = cat_correct / cat_total if cat_total > 0 else 0

        results = {
            'overall': accuracy,
            'categories': category_acc
        }

        # ä¿å­˜è¯¦ç»†è¯„æµ‹ç»“æœ
        score_file = eval_file.replace('.xlsx', '_score.xlsx')
        data.to_excel(score_file)

        return pd.DataFrame([results])

    @staticmethod
    def extract_single_choice_with_word_boundary(pred):
        """
        ä»é¢„æµ‹æ–‡æœ¬ä¸­æå–é€‰é¡¹ï¼Œå¹¶ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒã€‚
        è¿”å›æå–åˆ°çš„é€‰é¡¹ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›Noneã€‚
        """
        if pred is None:
            return None

        # ç¡®ä¿predæ˜¯å­—ç¬¦ä¸²ç±»å‹
        try:
            pred = str(pred)
        except Exception:
            return None

        pattern_1 = r'``([^`]*)``'
        match = re.search(pattern_1, pred)
        if match:
            pred = match.group(1)  # æå–åå¼•å·ä¹‹é—´çš„å†…å®¹

        pattern_2 = r'`([^`]*)`'
        match = re.search(pattern_2, pred)
        if match:
            pred = match.group(1)  # æå–åŒåå¼•å·ä¹‹é—´çš„å†…å®¹

        pattern_3 = r'\b[A-D]\b(?!\s[a-zA-Z])'
        match = re.search(pattern_3, pred)
        if match:
            pred = match.group()  # æå–å­¤ç«‹çš„å¤§å†™å­—æ¯ï¼ˆæ’é™¤"A bike"ï¼Œä¸å®šå† è¯+ç©ºæ ¼+å•è¯çš„æƒ…å†µï¼‰
        else:
            return None  # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å› None

        return pred

    @staticmethod
    def extract_single_choice_with_llm(pred, question=None, cache_dir=None):
        """
        Extract a single choice answer from a prediction using an LLM.
        Uses concurrency and caching to improve performance.

        Args:
            pred (str): The prediction text to extract a choice from
            question (str, optional): The question text containing options. Default is None.
            cache_dir (str, optional): Directory for caching results. Default is '.cache'.

        Returns:
            str: The extracted choice (A, B, C, D, or Z for no match)
        """
        import hashlib
        import os
        import json
        from concurrent.futures import ProcessPoolExecutor

        # Setup cache directory
        if cache_dir is None:
            cache_dir = os.environ.get('.cache', '.cache')

        os.makedirs(cache_dir, exist_ok=True)

        # Create cache key from question and prediction
        def get_cache_path(question, pred):
            combined = (question or "") + (pred or "")
            hash_key = hashlib.md5(combined.encode()).hexdigest()
            return os.path.join(cache_dir, f"choice_cache_{hash_key}.json")

        cache_path = get_cache_path(question, pred)

        # Check cache first
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r') as f:
                    cached_data = json.load(f)
                    return cached_data.get('choice', 'Z')
            except Exception:
                # If there's any error with the cache, proceed without it
                pass

        # Build the model using build_judge
        model = build_judge(model='chatgpt-0125')

        # Build the prompt for the LLM
        prompt = (
            'You are an AI assistant who will help me to match '
            'an answer with several options of a single-choice question. '
            'You are provided with a question and an answer, '
            'and you need to find which option is most similar to the answer. '
            'If the meaning of all options are significantly different from the answer, output Z. '
            'Your should output a single uppercase character in A, B, C, D (if they are valid options), and Z. '
            'Do not explain your reasoning, just output the letter directly.\n'
            'Example 1: \n'
            'Question: What is the main object in image?\nOptions: A. teddy bear B. rabbit C. cat D. dog\n'
            'Answer: a cute teddy bear\nYour output: A\n'
            'Example 2: \n'
            'Question: What is the main object in image?\nOptions: A. teddy bear B. rabbit C. cat D. dog\n'
            'Answer: Spider\nYour output: Z\n'
            'Example 3: \n'
            f'Question: {question}\nAnswer: {pred}\nYour output: '
        )

        retry = 3
        while retry:
            try:
                ans = model.generate(prompt)
                # Try to extract the choice from the answer
                choice = None

                # First look for a single letter answer
                match = re.search(r'\b([A-DZ])\b', ans)
                if match:
                    choice = match.group(1)

                # Also look for patterns like "(A)" or "A."
                if not choice:
                    match = re.search(r'[\(\s]([A-DZ])[\)\.\s]', ans)
                    if match:
                        choice = match.group(1)

                if choice and choice in "ABCDZ":
                    # Save to cache
                    try:
                        with open(cache_path, 'w') as f:
                            json.dump({
                                'choice': choice,
                                'full_response': ans,
                                'prompt': prompt
                            }, f)
                    except Exception:
                        # If caching fails, just continue
                        pass

                    return choice
            except Exception:
                pass

            retry -= 1

        # If all attempts failed, return Z
        # Save failure to cache too
        try:
            with open(cache_path, 'w') as f:
                json.dump({
                    'choice': 'Z',
                    'full_response': 'Failed to extract',
                    'prompt': prompt
                }, f)
        except Exception:
            pass

        return "Z"

    @staticmethod
    def _process_single_item(row_dict, cache_dir='.cache'):
        """
        å¤„ç†å•ä¸ªé¡¹ç›®çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†

        Args:
            row_dict (dict): æ•°æ®è¡Œå­—å…¸
            cache_dir (str): ç¼“å­˜ç›®å½•

        Returns:
            str: æå–çš„é€‰é¡¹
        """
        import time
        import random
        import sys
        import os
        import hashlib
        import json
        import re

        try:
            pred = row_dict['prediction']
            question = row_dict['question']

            # æ„å»ºç¼“å­˜è·¯å¾„
            def get_cache_path(question, pred):
                # è§„èŒƒåŒ–è¾“å…¥ï¼Œåˆ é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆç›¸åŒçš„å“ˆå¸Œå€¼
                combined = ((question or "") + (pred or "")).strip()
                # ä»…ä½¿ç”¨å‰1000ä¸ªå­—ç¬¦è®¡ç®—å“ˆå¸Œï¼Œé¿å…è¶…é•¿æ–‡æœ¬
                combined = combined[:1000]
                hash_key = hashlib.md5(combined.encode('utf-8', errors='ignore')).hexdigest()
                return os.path.join(cache_dir, f"choice_cache_{hash_key}.json")

            cache_path = get_cache_path(question, pred)

            # æ£€æŸ¥ç¼“å­˜ - æ·»åŠ è¯¦ç»†æ—¥å¿—
            if os.path.exists(cache_path):
                try:
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                        choice = cached_data.get('choice', None)
                        if choice and choice in "ABCDZ":
                            print(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_path}")
                            return choice
                        else:
                            print(f"âŒ ç¼“å­˜æ ¼å¼æ— æ•ˆ: {cache_path}")
                except Exception as e:
                    print(f"âŒ è¯»å–ç¼“å­˜å‡ºé”™ {cache_path}: {e}")
            else:
                print(f"âš ï¸ ç¼“å­˜ä¸å­˜åœ¨: {cache_path}")

            # æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…APIé€Ÿç‡é™åˆ¶
            time.sleep(random.uniform(0, 0.2))

            # ç›´æ¥è°ƒç”¨é™æ€æ–¹æ³•å¤„ç†
            from .utils import build_judge

            # æ„å»ºæ¨¡å‹
            model = build_judge(model='chatgpt-0125')

            # æ„å»ºæç¤º
            prompt = (
                'You are an AI assistant who will help me to match '
                'an answer with several options of a single-choice question. '
                'You are provided with a question and an answer, '
                'and you need to find which option is most similar to the answer. '
                'If the meaning of all options are significantly different from the answer, output Z. '
                'Your should output a single uppercase character in A, B, C, D (if they are valid options), and Z. '
                'Do not explain your reasoning, just output the letter directly.\n'
                'Example 1: \n'
                'Question: What is the main object in image?\nOptions: A. teddy bear B. rabbit C. cat D. dog\n'
                'Answer: a cute teddy bear\nYour output: A\n'
                'Example 2: \n'
                'Question: What is the main object in image?\nOptions: A. teddy bear B. rabbit C. cat D. dog\n'
                'Answer: Spider\nYour output: Z\n'
                'Example 3: \n'
                f'Question: {question}\nAnswer: {pred}\nYour output: '
            )

            # è°ƒç”¨æ¨¡å‹
            retry = 3
            while retry:
                try:
                    ans = model.generate(prompt)
                    # æå–é€‰é¡¹
                    choice = None

                    # é¦–å…ˆæŸ¥æ‰¾å•ä¸ªå­—æ¯ç­”æ¡ˆ
                    match = re.search(r'\b([A-DZ])\b', ans)
                    if match:
                        choice = match.group(1)

                    # è¿˜æŸ¥æ‰¾ç±»ä¼¼"(A)"æˆ–"A."çš„æ¨¡å¼
                    if not choice:
                        match = re.search(r'[\(\s]([A-DZ])[\)\.\s]', ans)
                        if match:
                            choice = match.group(1)

                    if choice and choice in "ABCDZ":
                        # ä¿å­˜åˆ°ç¼“å­˜
                        try:
                            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                            with open(cache_path, 'w', encoding='utf-8') as f:
                                cache_data = {
                                    'choice': choice,
                                    'full_response': ans,
                                    'prompt': prompt
                                }
                                json.dump(cache_data, f, ensure_ascii=False)
                                print(f"âœ… ç¼“å­˜å·²ä¿å­˜: {cache_path}")
                        except Exception as e:
                            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ {cache_path}: {e}")

                        return choice
                except Exception as e:
                    print(f"âŒ è°ƒç”¨æ¨¡å‹å‡ºé”™: {e}")

                retry -= 1

            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›Z
            # ä¿å­˜å¤±è´¥ç»“æœåˆ°ç¼“å­˜
            try:
                with open(cache_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'choice': 'Z',
                        'full_response': 'è°ƒç”¨å¤±è´¥',
                        'prompt': prompt
                    }, f, ensure_ascii=False)
                    print(f"âš ï¸ ä¿å­˜é»˜è®¤ç¼“å­˜(Z): {cache_path}")
            except Exception as e:
                print(f"âŒ ä¿å­˜é»˜è®¤ç¼“å­˜å¤±è´¥: {e}")

            return "Z"
        except Exception as e:
            print(f"âŒ å¤„ç†é¡¹ç›®æ—¶å‡ºé”™: {e}")
            return "Z"  # å‡ºé”™æ—¶è¿”å›é»˜è®¤å€¼

    @staticmethod
    def batch_extract_choices_with_llm(data_rows, num_processes=32, cache_dir='.cache',
                                       use_single_thread=False):
        """
        å¹¶å‘å¤„ç†å¤šä¸ªé¢„æµ‹

        Args:
            data_rows: åŒ…å«é¢„æµ‹å’Œé—®é¢˜çš„æ•°æ®è¡Œ
            num_processes: è¦ä½¿ç”¨çš„å¹¶å‘è¿›ç¨‹æ•°
            cache_dir: ç¼“å­˜ç»“æœçš„ç›®å½•
            use_single_thread: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹å¤„ç†

        Returns:
            list: æå–çš„é€‰é¡¹åˆ—è¡¨
        """
        import os
        import multiprocessing as mp
        from functools import partial
        import time

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)

        # ä½¿ç”¨è¿›ç¨‹æ± æ¥å¹¶å‘å¤„ç†é¡¹ç›®
        results = []
        total = len(data_rows)

        # ä¸ºæ¯è¡Œåˆ›å»ºç®€å•çš„å­—å…¸ï¼ŒåªåŒ…å«å¿…è¦ä¿¡æ¯
        row_dicts = []
        for row in data_rows:
            row_dicts.append({
                'prediction': row['prediction'],
                'question': row['question']
            })

        # å¦‚æœæŒ‡å®šäº†å•çº¿ç¨‹æˆ–è¿›ç¨‹æ•°ä¸º1ï¼Œåˆ™ä½¿ç”¨å•çº¿ç¨‹å¤„ç†
        if use_single_thread or num_processes <= 1:
            print(f"ä½¿ç”¨å•çº¿ç¨‹å¤„ç† {total} ä¸ªæ ·æœ¬...")
            results = []
            start_time = time.time()
            for i, row in enumerate(row_dicts):
                result = MMSIBenchDataset._process_single_item(row, cache_dir=cache_dir)
                results.append(result)
                if (i + 1) % max(1, total // 20) == 0:
                    elapsed = time.time() - start_time
                    remaining = (elapsed / (i + 1)) * (total - (i + 1))
                    print(f"å•çº¿ç¨‹è¿›åº¦: {i + 1}/{total} ({(i + 1) / total * 100:.1f}%) - "
                          f"å·²ç”¨æ—¶: {elapsed:.1f}ç§’, å‰©ä½™æ—¶é—´: {remaining:.1f}ç§’")
            return results

        # åˆ›å»ºè¿›ç¨‹å…±äº«çš„å¤„ç†å‡½æ•°
        process_func = partial(MMSIBenchDataset._process_single_item, cache_dir=cache_dir)

        # å¯¹äºWindowsï¼Œéœ€è¦ä½¿ç”¨if __name__=='__main__'æ¥é¿å…å­è¿›ç¨‹é€’å½’åˆ›å»º
        # ä½†åœ¨å½“å‰ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨multiprocessing
        print(f"å¯åŠ¨ {num_processes} ä¸ªè¿›ç¨‹å¤„ç† {total} ä¸ªæ ·æœ¬")

        try:
            # è®¾ç½®å¯åŠ¨æ–¹æ³•ä¸º forkserver æˆ– spawn ä»¥æé«˜å…¼å®¹æ€§
            if hasattr(mp, 'get_context'):
                try:
                    # ä¼˜å…ˆä½¿ç”¨ forkserverï¼Œå®ƒåœ¨Linuxä¸Šé€šå¸¸æ›´å¯é 
                    ctx = mp.get_context('forkserver')
                    pool = ctx.Pool(processes=num_processes)
                    print("ä½¿ç”¨ forkserver æ–¹å¼å¯åŠ¨è¿›ç¨‹æ± ")
                except ValueError:
                    try:
                        # å¦‚æœä¸æ”¯æŒ forkserverï¼Œåˆ™å°è¯• spawn
                        ctx = mp.get_context('spawn')
                        pool = ctx.Pool(processes=num_processes)
                        print("ä½¿ç”¨ spawn æ–¹å¼å¯åŠ¨è¿›ç¨‹æ± ")
                    except ValueError:
                        # å¦‚æœéƒ½ä¸æ”¯æŒï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ–¹å¼
                        pool = mp.Pool(processes=num_processes)
                        print("ä½¿ç”¨é»˜è®¤æ–¹å¼å¯åŠ¨è¿›ç¨‹æ± ")
            else:
                # å¦‚æœæ— æ³•è®¾ç½®ä¸Šä¸‹æ–‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ± 
                pool = mp.Pool(processes=num_processes)
                print("ä½¿ç”¨æ ‡å‡†è¿›ç¨‹æ± ")

            # ä½¿ç”¨ imap å¯ä»¥æŒ‰é¡ºåºå¾—åˆ°ç»“æœï¼ŒåŒæ—¶æ”¯æŒå¹¶è¡Œå¤„ç†
            chunksize = max(1, total // (num_processes * 4))
            for i, result in enumerate(pool.imap(process_func, row_dicts, chunksize=chunksize)):
                results.append(result)
                processed = i + 1
                if processed % max(1, total // 20) == 0:  # æ¯5%æ›´æ–°ä¸€æ¬¡è¿›åº¦
                    print(f"è¿›åº¦: {processed}/{total} ({processed / total * 100:.1f}%)")

            pool.close()
            pool.join()

        except Exception as e:
            print(f"å¹¶è¡Œå¤„ç†å‡ºé”™: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå°è¯•å•çº¿ç¨‹å¤„ç†
            print("å°è¯•å•çº¿ç¨‹å¤„ç†...")
            results = []
            for i, row in enumerate(row_dicts):
                result = MMSIBenchDataset._process_single_item(row, cache_dir=cache_dir)
                results.append(result)
                if (i + 1) % max(1, total // 20) == 0:
                    print(f"å•çº¿ç¨‹è¿›åº¦: {i + 1}/{total} ({(i + 1) / total * 100:.1f}%)")

        print(f"å®Œæˆæ‰€æœ‰å¤„ç†: {len(results)}/{total}")
        return results


class MMSIBenchCircular(MMSIBenchDataset):
    """
    MMSI Bench Circular Dataset class.
    Uses circular evaluation method for multiple-choice questions.
    é€‰é¡¹åµŒå…¥åœ¨questionå­—æ®µä¸­ï¼Œä½¿ç”¨circular evaluationæ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚
    """
    TYPE = 'MCQ'

    @classmethod
    def supported_datasets(cls):
        return ['MMSI_Bench_Circular']

    def extract_options_from_question(self, question):
        """
        ä»questionæ–‡æœ¬ä¸­æå–é€‰é¡¹ï¼Œè¿”å›(ä¸»é—®é¢˜æ–‡æœ¬, é€‰é¡¹dict)
        """
        # åˆ†å‰²å‡º Options: åé¢çš„å†…å®¹
        parts = question.split("Options:", 1)
        if len(parts) < 2:
            return question.strip(), {}

        question_text = parts[0].strip()
        options_text = parts[1].strip()

        # é€šç”¨çš„é€‰é¡¹æå–æ¨¡å¼ï¼Œé€‚ç”¨äºé€—å·åˆ†éš”æˆ–ç©ºæ ¼åˆ†éš”çš„æƒ…å†µ
        pattern = r'([A-D])\s*:\s*(.*?)(?=\s+[A-D]\s*:|,\s*[A-D]\s*:|$)'
        matches = re.findall(pattern, options_text)
        options = {m[0]: m[1].strip() for m in matches}

        return question_text, options

    def build_question_with_options(self, question_text, options):
        """
        é‡æ–°æ‹¼æ¥questionå’Œoptionsä¸ºåŸæ ¼å¼
        """
        options_str = "Options: " + ", ".join([f"{k}: {v}" for k, v in options.items()])
        return f"{question_text}\n{options_str}"

    def load_data(self, dataset):
        """
        åŠ è½½æ•°æ®å¹¶è‡ªåŠ¨ç”Ÿæˆ circular å˜ä½“ï¼Œæ¯é¢˜4ç§é€‰é¡¹é¡ºåºã€‚
        """
        if dataset == 'MMSI_Bench_Circular':
            # ä½¿ç”¨çˆ¶ç±»çš„ç½‘ç»œä¸‹è½½æ–¹æ³•åŠ è½½MMSI_Benchæ•°æ®
            data = super(MMSIBenchCircular, self).load_data('MMSI_Bench')
            assert 'index' in data.columns, "TSVæ–‡ä»¶ç¼ºå°‘'index'åˆ—"
            assert 'question' in data.columns, "TSVæ–‡ä»¶ç¼ºå°‘'question'åˆ—"

            cp4 = ['ABCD', 'BCDA', 'CDAB', 'DABC']
            new_rows = []

            for _, row in tqdm(data.iterrows(), desc="Processing data", total=len(data)):
                question_text, options = self.extract_options_from_question(row['question'])
                answer = row['answer'] if 'answer' in row else None

                # è·³è¿‡æ²¡æœ‰é€‰é¡¹çš„é¢˜
                if not options or answer not in options:
                    # import ipdb; ipdb.set_trace()
                    print(f"è·³è¿‡æ²¡æœ‰é€‰é¡¹çš„é¢˜: {row['index']}")
                    print(f"é€‰é¡¹: {options}")
                    print(f"ç­”æ¡ˆ: {answer}")
                    print(f"row['question']: {row['question']}")
                    continue

                for i, order in enumerate(cp4):
                    # é‡æ–°æ’åˆ—é€‰é¡¹
                    new_options = {k: options[o] for k, o in zip('ABCD', order) if o in options}
                    # è®¡ç®—æ–°ç­”æ¡ˆ
                    if answer in order:
                        new_answer = 'ABCD'[order.index(answer)]
                    else:
                        new_answer = answer  # fallback

                    # æ„é€ æ–°è¡Œ
                    new_row = row.copy()
                    # é‡æ–°æ‹¼æ¥question
                    new_row['question'] = self.build_question_with_options(question_text, new_options)
                    new_row['answer'] = new_answer
                    new_row['index'] = int(row['index']) + i * 1000000
                    new_row['g_index'] = row['index']  # ç”¨äºåˆ†ç»„
                    new_rows.append(new_row)

            new_data = pd.DataFrame(new_rows)
            return new_data

        else:
            return super(MMSIBenchCircular, self).load_data(dataset)

    def evaluate(self, eval_file, cache_dir='.cache', num_processes=32, use_single_thread=False,
                 **judge_kwargs):
        """
        è¯„ä¼°æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—å¾ªç¯è¯„ä¼°å’Œä¼ ç»Ÿè¯„ä¼°çš„ç»“æœ

        Args:
            eval_file: è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            num_processes: å¹¶è¡Œå¤„ç†çš„è¿›ç¨‹æ•°
            use_single_thread: æ˜¯å¦ä½¿ç”¨å•çº¿ç¨‹å¤„ç†
            **judge_kwargs: å…¶ä»–å‚æ•°
        """
        from .utils.multiple_choice import report_acc
        import pandas as pd
        import numpy as np
        import os

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)

        # æ£€æŸ¥ç¼“å­˜ç›®å½•ä¸­å·²æœ‰çš„ç¼“å­˜æ–‡ä»¶æ•°é‡
        cache_files = glob.glob(os.path.join(cache_dir, "choice_cache_*.json"))
        print(f"ç¼“å­˜ç›®å½• '{cache_dir}' ä¸­å·²æœ‰ {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")

        suffix = eval_file.split('.')[-1]

        # åŠ è½½å’Œé¢„å¤„ç†è¯„ä¼°æ•°æ®
        data = load(eval_file)
        data = data.sort_values(by='index')
        data['index'] = [int(x) for x in data['index']]
        data['prediction'] = [str(x) for x in data['prediction']]

        # ç¡®ä¿æ•°æ®ä¸­æœ‰g_indexå­—æ®µ
        if 'g_index' not in data.columns:
            data['g_index'] = [int(x % 1e6) for x in data['index']]

        # ä½¿ç”¨LLMæå–é€‰é¡¹ï¼Œé€šè¿‡å¹¶å‘å¤„ç†æé«˜é€Ÿåº¦
        print(f"ä½¿ç”¨ {num_processes} ä¸ªå¹¶è¡Œè¿›ç¨‹æå–é€‰é¡¹ï¼Œç¼“å­˜ç›®å½•ï¼š{cache_dir}")
        print("ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼" if use_single_thread else "ä½¿ç”¨å¤šè¿›ç¨‹æ¨¡å¼")

        # æå–å‰ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶æ•°é‡
        cache_files_before = len(glob.glob(os.path.join(cache_dir, "choice_cache_*.json")))
        
        # è¿™é‡Œå¯ä»¥åˆ‡æ¢ç”¨å“ªç§æ–¹å¼æ¥æå–ç­”æ¡ˆ
        # å¯¹äºå¤§å¤šæ•°modelï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯ extract_single_choice_with_word_boundary
        # å¯¹äº Claude3-7V_Sonnetã€Llama-3.2-11B-Vision-Instructã€doubao-1-5-thinking-vision-pro-250428
        # æˆ‘ä»¬ç”¨çš„æ˜¯ LLM æ¥æå–ç­”æ¡ˆï¼ˆbatch_extract_choices_with_llmï¼‰
        # é€šè¿‡æ³¨é‡Š/å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä¸¤è¡Œæ¥åˆ‡æ¢

        # --- ç”¨ LLM æå–ç­”æ¡ˆï¼ˆé€‚ç”¨äºéƒ¨åˆ†æ¨¡å‹ï¼Œè§ä¸Šæ³¨é‡Šï¼‰---
        # data['extracted_pred'] = MMSIBenchDataset.batch_extract_choices_with_llm(
        #     data.to_dict('records'), 
        #     num_processes=num_processes,
        #     cache_dir=cache_dir,
        #     use_single_thread=use_single_thread
        # )

        # --- ç”¨æ­£åˆ™ç²¾ç¡®åŒ¹é…æå–ç­”æ¡ˆï¼ˆå¤§å¤šæ•°æ¨¡å‹ç”¨è¿™ä¸ªï¼‰---
        data['extracted_pred'] = data['prediction'].apply(MMSIBenchDataset.extract_single_choice_with_word_boundary)
        # æå–åç»Ÿè®¡ç¼“å­˜æ–‡ä»¶æ•°é‡
        cache_files_after = len(glob.glob(os.path.join(cache_dir, "choice_cache_*.json")))
        new_cache_files = cache_files_after - cache_files_before

        print(f"å·²å®Œæˆ {len(data)} ä¸ªæ ·æœ¬çš„é€‰é¡¹æå–")
        print(f"æ–°å¢ {new_cache_files} ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œå…±æœ‰ {cache_files_after} ä¸ªç¼“å­˜æ–‡ä»¶")

        # ----- å¾ªç¯è¯„ä¼° (Circular Evaluation) -----
        print("ğŸ”„ å¼€å§‹è®¡ç®—å¾ªç¯è¯„ä¼° (Circular Evaluation) ç»“æœ...")

        # åˆ†ç»„è¯„ä¼°
        groups = data.groupby('g_index')
        circular_results = []

        for g_index, group in tqdm(groups, desc="Processing groups", total=len(groups)):
            # éªŒè¯ g_index ç¡®å®å°äº 10^6
            assert g_index < 1e6, f"g_index {g_index} ä¸å°äº 10^6ï¼Œæ•°æ®æœ‰è¯¯"

            # åˆ›å»ºåŸºæœ¬ç»“æœè¡Œ
            result_row = {
                'index': int(g_index),  # ä½¿ç”¨g_indexä½œä¸ºä¸»index
                'category': group['category'].iloc[0],
                'hit': 0,
                'log': ''
            }

            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢„æµ‹éƒ½æ­£ç¡®
            all_correct = True
            log_parts = []

            for _, row in group.iterrows():
                pred = row['extracted_pred']
                ans = row['answer']

                # è®°å½•å½“å‰è¡Œé¢„æµ‹ç»“æœ
                correct = (pred == ans)
                log_parts.append(f"Index {row['index']}: é¢„æµ‹={pred}, ç­”æ¡ˆ={ans}, æ­£ç¡®={correct}")

                if not correct:
                    all_correct = False

            # å¦‚æœæ‰€æœ‰é¢„æµ‹éƒ½æ­£ç¡®ï¼Œåˆ™è¿™é¢˜ç®—å¯¹
            result_row['hit'] = 1 if all_correct else 0
            result_row['log'] = '\n'.join(log_parts)

            circular_results.append(result_row)

        # åˆ›å»ºå¾ªç¯è¯„ä¼°ç»“æœDataFrame
        circular_df = pd.DataFrame(circular_results)

        # ----- ä¼ ç»Ÿè¯„ä¼° (Vanilla Evaluation) -----
        print("ğŸ” å¼€å§‹è®¡ç®—ä¼ ç»Ÿè¯„ä¼° (Vanilla Evaluation) ç»“æœ...")

        # åˆ›å»ºä¼ ç»Ÿè¯„ä¼°çš„ç»“æœåˆ—è¡¨
        vanilla_results = []

        for g_index, group in tqdm(groups, desc="Processing original items", total=len(groups)):
            # éªŒè¯ g_index ç¡®å®å°äº 10^6
            assert g_index < 1e6, f"g_index {g_index} ä¸å°äº 10^6ï¼Œæ•°æ®æœ‰è¯¯"

            # æ‰¾åˆ°ç»„å†…indexæœ€å°çš„è¡Œï¼ˆåŸå§‹é¢˜ç›®ï¼‰
            original_row = group.loc[group['index'].idxmin()]

            # åˆ›å»ºç»“æœè¡Œ
            result_row = {
                'index': int(g_index),  # ä½¿ç”¨g_indexä½œä¸ºä¸»index
                'category': original_row['category'],
                'answer': original_row['answer'],
                'prediction': original_row['prediction'],
                'extracted_pred': original_row['extracted_pred'],
                'hit': 1 if original_row['extracted_pred'] == original_row['answer'] else 0,
                'log': (f"Index {original_row['index']}: é¢„æµ‹={original_row['extracted_pred']}, "
                        f"ç­”æ¡ˆ={original_row['answer']}")
            }

            vanilla_results.append(result_row)

        # åˆ›å»ºä¼ ç»Ÿè¯„ä¼°ç»“æœDataFrame
        vanilla_df = pd.DataFrame(vanilla_results)

        # ----- ä¿å­˜ç»“æœ -----
        # ä¿å­˜å¾ªç¯è¯„ä¼°è¯¦ç»†ç»“æœ
        circular_detailed_file = eval_file.replace(f'.{suffix}', f'_circular_result.{suffix}')
        dump(circular_df, circular_detailed_file)

        # ä¿å­˜ä¼ ç»Ÿè¯„ä¼°è¯¦ç»†ç»“æœ
        vanilla_detailed_file = eval_file.replace(f'.{suffix}', f'_vanilla_result.{suffix}')
        dump(vanilla_df, vanilla_detailed_file)

        # è®¡ç®—å¾ªç¯è¯„ä¼°å‡†ç¡®ç‡
        circular_acc = {}
        circular_acc['Overall'] = np.mean(circular_df['hit'])

        # è®¡ç®—ä¼ ç»Ÿè¯„ä¼°å‡†ç¡®ç‡
        vanilla_acc = {}
        vanilla_acc['Overall'] = np.mean(vanilla_df['hit'])

        # æŒ‰ç±»åˆ«è®¡ç®—å‡†ç¡®ç‡
        if 'category' in circular_df.columns:
            categories = circular_df['category'].unique()
            for category in categories:
                # å¾ªç¯è¯„ä¼°
                circ_cat_data = circular_df[circular_df['category'] == category]
                circular_acc[category] = np.mean(circ_cat_data['hit'])

                # ä¼ ç»Ÿè¯„ä¼°
                van_cat_data = vanilla_df[vanilla_df['category'] == category]
                vanilla_acc[category] = np.mean(van_cat_data['hit'])

        # åˆ›å»ºæŠ¥å‘Šæ ¼å¼
        combined_acc = {}

        # æ·»åŠ æ€»ä½“ç»“æœ
        combined_acc['Overall'] = {
            'Circular': circular_acc['Overall'],
            'Vanilla': vanilla_acc['Overall']
        }

        # æ·»åŠ å„ä¸ªç±»åˆ«çš„ç»“æœ
        for cat in categories:
            combined_acc[cat] = {
                'Circular': circular_acc[cat],
                'Vanilla': vanilla_acc[cat]
            }

        # ä½¿ç”¨æœ‰æ„ä¹‰çš„ç´¢å¼•åç§°åˆ›å»º DataFrame
        combined_df = pd.DataFrame(combined_acc).T
        combined_df.index.name = 'Category'

        # ç¡®ä¿ Overall åœ¨ç¬¬ä¸€è¡Œ
        if 'Overall' in combined_df.index:
            # è·å– Overall è¡Œçš„æ•°æ®
            overall_data = combined_df.loc['Overall']
            # åˆ é™¤åŸå§‹çš„ Overall è¡Œ
            combined_df = combined_df.drop('Overall')
            # ä½¿ç”¨ pd.concat å°† Overall è¡Œæ·»åŠ åˆ° DataFrame çš„å¼€å¤´
            combined_df = pd.concat([pd.DataFrame({'Circular': [overall_data['Circular']],
                                                   'Vanilla': [overall_data['Vanilla']]},
                                                  index=['Overall']),
                                     combined_df])

        # ä¿å­˜å‡†ç¡®ç‡ç»“æœ
        score_file = eval_file.replace(f'.{suffix}', '_combined_acc.csv')
        # ç¡®ä¿ä¿å­˜çš„CSVæ–‡ä»¶åŒ…å«è¡Œç´¢å¼•
        combined_df.to_csv(score_file)

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n====== MMSI_Bench è¯„æµ‹ç»“æœ ======")
        print(f"æ€»æ ·æœ¬ç»„æ•°: {len(circular_df)}")

        print(f"\nğŸ“Š ä¼ ç»Ÿè¯„ä¼° (Vanilla) - å•é¢˜æ­£ç¡®ç‡: {vanilla_acc['Overall']:.2%}")
        print(f"ğŸ“Š å¾ªç¯è¯„ä¼° (Circular) - å…¨é¢˜ç»„æ­£ç¡®ç‡: {circular_acc['Overall']:.2%}")

        print("\nğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for cat in categories:
            print(f"{cat}:")
            print(f"  ä¼ ç»Ÿè¯„ä¼°: {vanilla_acc[cat]:.2%}")
            print(f"  å¾ªç¯è¯„ä¼°: {circular_acc[cat]:.2%}")

        # è¿”å›ä¸¤ç§è¯„ä¼°ç»“æœçš„DataFrame
        return combined_df